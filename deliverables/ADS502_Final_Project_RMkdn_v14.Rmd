---
title: "ADS502-02-SP22 - Final Project"
author: "Carr_Aaron"
date: "04/18/2022"
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
- \usepackage{fvextra}
- \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
- \usepackage{pdflscape}
- \newcommand{\blandscape}{\begin{landscape}}
- \newcommand{\elandscape}{\end{landscape}}
---

# Final Project: Costa Rica Poverty Index Prediction (Using R)

# Data set up
## Set global knit options
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r global.options, include=TRUE}
knitr::opts_chunk$set(
  fig.align = 'center'
  )
```


## Load R libraries for global use & set seed
```{r}
library(dplyr)
library(ggplot2)
library(datasets)
library(scales)
library(C50)
library(gridExtra)
library(e1071)
library(BioStatR)
library(rpart)
library(rpart.plot)
library(nnet)
library(NeuralNetTools)
library(randomForest)
library(fastDummies)
library(caret)
library(corrplot)
library(car)
library(psych)
library(class)
library (ROCR)

set.seed(333)
```


## Load files into R dataframes
```{r}
# Read training data file into R df
train_df01 <- read.table(file = "502_Final_Train.csv", header = TRUE, sep = ",")

# Review first few rows
print(head(train_df01))

# Check df length
dim(train_df01)[1]

# Check training data set features for null values
sapply(train_df01, function(x) sum(is.na(x)))
```

```{r}
# Read test data file into R df
test_df01 <- read.table(file = "502_Final_Test.csv", header = TRUE, sep = ",")

# Review first few rows
print(head(test_df01))

# Check df length
dim(test_df01)[1]

# Check test data set features for null values
sapply(test_df01, function(x) sum(is.na(x)))
```


## Factorize training binary/categorical data
```{r}
train_df01$Target <- as.factor(train_df01$Target) 
train_df01$has_bathroom <- as.factor(train_df01$has_bathroom) 
train_df01$has_refrig <- as.factor(train_df01$has_refrig) 
train_df01$has_comp <- as.factor(train_df01$has_comp) 
train_df01$disabled <- as.factor(train_df01$disabled) 
train_df01$married <- as.factor(train_df01$married) 
train_df01$divorced <- as.factor(train_df01$divorced) 
train_df01$separated <- as.factor(train_df01$separated) 
train_df01$widower <- as.factor(train_df01$widower) 
train_df01$single <- as.factor(train_df01$single) 
train_df01$water_inside <- as.factor(train_df01$water_inside) 
train_df01$water_outside <- as.factor(train_df01$water_outside) 
train_df01$no_water <- as.factor(train_df01$no_water) 
train_df01$gov_elec <- as.factor(train_df01$gov_elec) 
train_df01$private_elec <- as.factor(train_df01$private_elec) 
train_df01$no_elec <- as.factor(train_df01$no_elec) 
train_df01$coop_elec <- as.factor(train_df01$coop_elec) 
train_df01$no_toilet <- as.factor(train_df01$no_toilet) 
train_df01$sewer <- as.factor(train_df01$sewer) 
train_df01$septictank <- as.factor(train_df01$septictank) 
train_df01$latrine <- as.factor(train_df01$latrine) 
train_df01$toilet_other <- as.factor(train_df01$toilet_other) 
train_df01$no_cooking_energy <- as.factor(train_df01$no_cooking_energy) 
train_df01$cooking_.elec <- as.factor(train_df01$cooking_.elec) 
train_df01$cooking_gas <- as.factor(train_df01$cooking_gas) 
train_df01$cooking_woodcoal <- as.factor(train_df01$cooking_woodcoal) 
train_df01$trash_truck <- as.factor(train_df01$trash_truck) 
train_df01$trash_buried <- as.factor(train_df01$trash_buried) 
train_df01$trash_burning <- as.factor(train_df01$trash_burning) 
train_df01$trash_throwing <- as.factor(train_df01$trash_throwing) 
train_df01$trash_river <- as.factor(train_df01$trash_river) 
```


## Factorize test binary/categorical data
```{r}
test_df01$Target <- as.factor(test_df01$Target) 
test_df01$has_bathroom <- as.factor(test_df01$has_bathroom) 
test_df01$has_refrig <- as.factor(test_df01$has_refrig) 
test_df01$has_comp <- as.factor(test_df01$has_comp) 
test_df01$disabled <- as.factor(test_df01$disabled) 
test_df01$married <- as.factor(test_df01$married) 
test_df01$divorced <- as.factor(test_df01$divorced) 
test_df01$separated <- as.factor(test_df01$separated) 
test_df01$widower <- as.factor(test_df01$widower) 
test_df01$single <- as.factor(test_df01$single) 
test_df01$water_inside <- as.factor(test_df01$water_inside) 
test_df01$water_outside <- as.factor(test_df01$water_outside) 
test_df01$no_water <- as.factor(test_df01$no_water) 
test_df01$gov_elec <- as.factor(test_df01$gov_elec) 
test_df01$private_elec <- as.factor(test_df01$private_elec) 
test_df01$no_elec <- as.factor(test_df01$no_elec) 
test_df01$coop_elec <- as.factor(test_df01$coop_elec) 
test_df01$no_toilet <- as.factor(test_df01$no_toilet) 
test_df01$sewer <- as.factor(test_df01$sewer) 
test_df01$septictank <- as.factor(test_df01$septictank) 
test_df01$latrine <- as.factor(test_df01$latrine) 
test_df01$toilet_other <- as.factor(test_df01$toilet_other) 
test_df01$no_cooking_energy <- as.factor(test_df01$no_cooking_energy) 
test_df01$cooking_.elec <- as.factor(test_df01$cooking_.elec) 
test_df01$cooking_gas <- as.factor(test_df01$cooking_gas) 
test_df01$cooking_woodcoal <- as.factor(test_df01$cooking_woodcoal) 
test_df01$trash_truck <- as.factor(test_df01$trash_truck) 
test_df01$trash_buried <- as.factor(test_df01$trash_buried) 
test_df01$trash_burning <- as.factor(test_df01$trash_burning) 
test_df01$trash_throwing <- as.factor(test_df01$trash_throwing) 
test_df01$trash_river <- as.factor(test_df01$trash_river) 
```


## Parse columns into different sub df's for modeling
```{r}
y01_lst <- c("Target")
x01_lst <- c("num_children",
                 "cooking_gas",
                 "trash_truck",
                 "dependency",
                 "sewer",
                 "total_persons"
                 )
xy01_lst <- c("Target",
                  "num_children",
                  "cooking_gas",
                  "trash_truck",
                  "dependency",
                  "sewer",
                  "total_persons"
                  )

train_x01_df01 <- subset(x = train_df01, select = x01_lst)
train_xy01_df01 <- subset(x = train_df01, select = xy01_lst)
test_x01_df01 <- subset(x = test_df01, select = x01_lst)
test_xy01_df01 <- subset(x = test_df01, select = xy01_lst)
```


# Exploratory Data Analysis (EDA)
## Create boxplots for continuous variables
```{r, fig.height=5, fig.width=10}
# Define function to produce formatted boxplots
box_comp <- function(xcol = NA, df = NA) {
df_s1 <- df[, xcol]

# Calculate quartiles
xcol_iqr_lim <- IQR(df_s1) * 1.5
xcol_q1 <- quantile(df_s1, probs = c(.25))
xcol_otlow <- xcol_q1 - xcol_iqr_lim

xcol_q3 <- quantile(df_s1, probs = c(.75))
xcol_othigh <- xcol_q3 + xcol_iqr_lim

# Subset non-outlier data 
xcol_iqr_df01 <- subset(df, (df_s1 > xcol_otlow & df_s1 < xcol_othigh))
df_s2 <- xcol_iqr_df01[, xcol]

# Begin calculating measures of centrality & dispersion
xcol_mean <- round(mean(df_s1), 3)
xcol_iqr_df01_trunc_mean <- round(mean(df_s2), 3)
xcol_med <- median(df_s1)
xcol_iqr_df01_trunc_med <- median(df_s2)
xcol_mode <- mode(df_s1)
xcol_iqr_df01_trunc_mode <- mode(df_s2)
xcol_stde <- round(sd(df_s1), 3)
xcol_iqr_df01_trunc_stde <- round(sd(df_s2), 3)
xcol_vari <- round(var(df_s1), 3)
xcol_iqr_df01_trunc_vari <- round(var(df_s2), 3)

var01_min <- min(df[, xcol])
var01_max <- max(df[, xcol])
var01_range <- var01_max - var01_min
  
var02_min <- min(xcol_iqr_df01[, xcol])
var02_max <- max(xcol_iqr_df01[, xcol])
var02_range <- var02_max - var02_min

# Configure y-axis min & max to sync graphs
plot_min <- min(var01_min, var02_min)
plot_max <- max(var01_max, var02_max)

# Plot 2 boxplots as comparison of full data set vs. w/o outliers
ggp1 <- ggplot(df, aes_string(y = xcol)) +
  geom_boxplot() +
  ylim(plot_min, plot_max) +
  scale_x_continuous(labels = comma) +
  labs(title = paste0("Boxplot for Number of ", xcol), subtitle = "Outliers Included")

ggp2 <- ggplot(xcol_iqr_df01, aes_string(y = xcol)) +
  geom_boxplot() +
  ylim(plot_min, plot_max) + 
  scale_x_continuous(labels = comma) +
  labs(title = paste0("Boxplot for Number of ", xcol), subtitle = "Outliers from Full Dataset Excluded")

# Map 2 plots to 1 grid
grid.arrange(ggp1, ggp2, ncol = 2)  

# Define vectors for a measurement table df
measure_name <- c(paste0("Variable: ", xcol),
                  "Count",
                  "NA Count",
                  "Mean",
                  "Median",
                  "Mode",
                  "Standard Deviation",
                  "Variance",
                  "Range",
                  "Min",
                  "Max",
                  "IQR Outlier Limit",
                  "25th Percentile",
                  "75th Percentile",
                  "Lower Outlier Threshold",
                  "Upper Outlier Threshold"
                  )

measure_val01 <- c("All data points",
                   as.character(dim(df)[1]),
                   sum(is.na(df_s1)),
                   xcol_mean,
                   xcol_med,
                   xcol_mode,
                   xcol_stde,
                   xcol_vari,
                   var01_range,
                   var01_min,
                   var01_max,
                   paste0("+/-", xcol_iqr_lim),
                   xcol_q1,
                   xcol_q3,
                   xcol_otlow,
                   xcol_othigh
                   )

measure_val02 <- c("Outliers Excluded",
                   paste0(as.character(dim(xcol_iqr_df01)[1]), " (",
                          round((as.numeric(dim(xcol_iqr_df01)[1] / as.numeric(dim(df)[1]))) * 100, 1),
                          "%)"
                          ),
                   sum(is.na(df_s2)),
                   xcol_iqr_df01_trunc_mean,
                   xcol_iqr_df01_trunc_med,
                   xcol_iqr_df01_trunc_mode,
                   xcol_iqr_df01_trunc_stde,
                   xcol_iqr_df01_trunc_vari,
                   var02_range,
                   var02_min,
                   var02_max,
                   "-",
                   "-",
                   "-",
                   "-",
                   "-"
                   )

measure_tbl <- data.frame(measure_name, measure_val01, measure_val02)
print(measure_tbl)
}

# Run function on all continuous variables in training data set
box_comp(xcol = "total_persons", df = train_df01)
box_comp(xcol = "num_phones", df = train_df01)
box_comp(xcol = "num_children", df = train_df01)
box_comp(xcol = "num_65", df = train_df01)
box_comp(xcol = "num_adults", df = train_df01)
box_comp(xcol = "age", df = train_df01)
box_comp(xcol = "dependency", df = train_df01)
```


## Create bar charts for categorical variables
```{r}
# Define function to produce formatted bar charts
plot_bar <- function(x = NA, y = NA, df = NA) {

# Contingency table for `marital` and `response`
cross_tab_df01 <- table(df[, y], df[, x])

cross_tab_df02 <- addmargins(A = cross_tab_df01, FUN = list(total = sum),
                             quiet = TRUE)
cross_tab_df03 <- round(prop.table(cross_tab_df01, margin = 2) * 100, 1)
cross_tab_df04 <- addmargins(A = cross_tab_df03, FUN = list(total = sum),
                             quiet = TRUE)
print(paste0("Contingency Table for ", y, " and ", x))
print(cross_tab_df02)
print(paste0("Proportions Contingency Table for ", y, " and ", x))
print(cross_tab_df04)

# Plot bar graphs
ggplot(df, aes_string(x)) +
  geom_bar(aes_string(fill = y)) +
  labs(title = paste0("Bar Graph of ", x, " Feature w/ ", y, " Class Overlay")) +
  theme(plot.title = element_text(hjust = 0.5, size = 12))
}

# Run function on all binary/categorical variables in training data set
plot_bar(x = "Target", y = "Target", df = train_df01)
plot_bar(x = "has_bathroom", y = "Target", df = train_df01)
plot_bar(x = "has_refrig", y = "Target", df = train_df01)
plot_bar(x = "has_comp", y = "Target", df = train_df01)
plot_bar(x = "disabled", y = "Target", df = train_df01)
plot_bar(x = "married", y = "Target", df = train_df01)
plot_bar(x = "divorced", y = "Target", df = train_df01)
plot_bar(x = "separated", y = "Target", df = train_df01)
plot_bar(x = "widower", y = "Target", df = train_df01)
plot_bar(x = "single", y = "Target", df = train_df01)
plot_bar(x = "water_inside", y = "Target", df = train_df01)
plot_bar(x = "water_outside", y = "Target", df = train_df01)
plot_bar(x = "no_water", y = "Target", df = train_df01)
plot_bar(x = "gov_elec", y = "Target", df = train_df01)
plot_bar(x = "private_elec", y = "Target", df = train_df01)
plot_bar(x = "no_elec", y = "Target", df = train_df01)
plot_bar(x = "coop_elec", y = "Target", df = train_df01)
plot_bar(x = "no_toilet", y = "Target", df = train_df01)
plot_bar(x = "sewer", y = "Target", df = train_df01)
plot_bar(x = "septictank", y = "Target", df = train_df01)
plot_bar(x = "latrine", y = "Target", df = train_df01)
plot_bar(x = "toilet_other", y = "Target", df = train_df01)
plot_bar(x = "no_cooking_energy", y = "Target", df = train_df01)
plot_bar(x = "cooking_.elec", y = "Target", df = train_df01)
plot_bar(x = "cooking_gas", y = "Target", df = train_df01)
plot_bar(x = "cooking_woodcoal", y = "Target", df = train_df01)
plot_bar(x = "trash_truck", y = "Target", df = train_df01)
plot_bar(x = "trash_buried", y = "Target", df = train_df01)
plot_bar(x = "trash_burning", y = "Target", df = train_df01)
plot_bar(x = "trash_throwing", y = "Target", df = train_df01)
plot_bar(x = "trash_river", y = "Target", df = train_df01)
```


## Parse non-binary numerical features into sub df's for EDA
```{r}
# Create sub df for numerical features
x01_num_lst <- c("total_persons",
                     "num_phones",
                     "num_children",
                     "num_65",
                     "num_adults",
                     "dependency",
                     "age"
                     )

train_x_num_df01_s1 <- subset(x = train_df01, select = x01_num_lst)
train_y_df01 <- subset(x = train_df01, select = c("Target"))
```


## Create scatterplots for continuous features
```{r, fig.height=10, fig.width=15}
# Scatterplot matrix
pairs(x = train_x_num_df01_s1, diag.panel = panel.hist)

# Individual scatterplots of numerical features included in modeling phase
spg1 <- ggplot(train_x_num_df01_s1, aes(num_children, dependency)) +
  geom_point() + 
  stat_smooth(method = "lm") +
  labs(title = "Scatterplot of num_children & dependency (w/ Linear Regression Line)")

spg2 <- ggplot(train_x_num_df01_s1, aes(num_children, total_persons)) +
  geom_point() + 
  stat_smooth(method = "lm") +
  labs(title = "Scatterplot of num_children & total_persons (w/ Linear Regression Line)")

spg3 <- ggplot(train_x_num_df01_s1, aes(dependency, total_persons)) +
  geom_point() + 
  stat_smooth(method = "lm") +
  labs(title = "Scatterplot of dependency & total_persons (w/ Linear Regression Line)")

spg1
spg2
spg3
```


## Correlation matrix for numerical features
```{r}
train_cormx_df01 <- cor(train_x_num_df01_s1)
print(train_cormx_df01)
corrplot(train_cormx_df01)

# Send correlation matrix df to file for converting to table
write.csv(train_cormx_df01, "corr_mat.csv")
```


# Model development using training set for evaluation on test data set
## Train & evaluate multiple classification models
* **Poverty Status Index (PSI) = 0/-:** An individual who is not vulnerable to being impoverished.
* **PSI = 1/+:** An individual who is either vulnerable to being impoverished, moderately impoverished, or extremely impoverished.

## **Model 1 (M1):** Random Forest Decision Trees (DTs)
```{r}
# Train model
train_m1v1_rf <- randomForest(Target~., data = train_xy01_df01)

# Use trained model to predict y on test data
test_m1v1_rf_y_pred_df01 <- predict(object = train_m1v1_rf, newdata = test_x01_df01, type = "class")

# Create contingency table to review results
test_m1v1_rf_y_pred_tbl01 <- table(test_df01$Target, test_m1v1_rf_y_pred_df01)
row.names(test_m1v1_rf_y_pred_tbl01) <- c("Actual: PSI = 0 (-)",
                                            "Actual: PSI = 1 (+)")
colnames(test_m1v1_rf_y_pred_tbl01) <- c("Predicted: PSI = 0 (-)",
                                           "Predicted: PSI = 1 (+)")
test_m1v1_rf_y_pred_tbl01 <- addmargins(A = test_m1v1_rf_y_pred_tbl01,
                                          FUN = list(Total = sum), quiet = TRUE)
print(test_m1v1_rf_y_pred_tbl01)

# Pull values from confusion matrix table
test_m1v1_rf_y_pred_tp01 <- test_m1v1_rf_y_pred_tbl01[2, 2]
test_m1v1_rf_y_pred_fn01 <- test_m1v1_rf_y_pred_tbl01[2, 1]
test_m1v1_rf_y_pred_fp01 <- test_m1v1_rf_y_pred_tbl01[1, 2]
test_m1v1_rf_y_pred_tn01 <- test_m1v1_rf_y_pred_tbl01[1, 1]
print(test_m1v1_rf_y_pred_tp01)
print(test_m1v1_rf_y_pred_fn01)
print(test_m1v1_rf_y_pred_fp01)
print(test_m1v1_rf_y_pred_tn01)

# ------------------------------------------------------------------------------
# Use trained model to predict y on train data for comparison
train_m1v1_rf_y_pred_df01 <- predict(object = train_m1v1_rf, newdata = train_x01_df01, type = "class")

# Create contingency table to review results
train_m1v1_rf_y_pred_tbl01 <- table(train_df01$Target, train_m1v1_rf_y_pred_df01)
row.names(train_m1v1_rf_y_pred_tbl01) <- c("Actual: PSI = 0 (-)",
                                             "Actual: PSI = 1 (+)")
colnames(train_m1v1_rf_y_pred_tbl01) <- c("Predicted: PSI = 0 (-)",
                                            "Predicted: PSI = 1 (+)")
train_m1v1_rf_y_pred_tbl01 <- addmargins(A = train_m1v1_rf_y_pred_tbl01,
                                           FUN = list(Total = sum), quiet = TRUE)
print(train_m1v1_rf_y_pred_tbl01)

# Pull values from confusion matrix table
train_m1v1_rf_y_pred_tp01 <- train_m1v1_rf_y_pred_tbl01[2, 2]
train_m1v1_rf_y_pred_fn01 <- train_m1v1_rf_y_pred_tbl01[2, 1]
train_m1v1_rf_y_pred_fp01 <- train_m1v1_rf_y_pred_tbl01[1, 2]
train_m1v1_rf_y_pred_tn01 <- train_m1v1_rf_y_pred_tbl01[1, 1]
print(train_m1v1_rf_y_pred_tp01)
print(train_m1v1_rf_y_pred_fn01)
print(train_m1v1_rf_y_pred_fp01)
print(train_m1v1_rf_y_pred_tn01)

# Generate classification evaluation measures
test_m1v1_rf_acc01 <- (test_m1v1_rf_y_pred_tp01 + test_m1v1_rf_y_pred_tn01) /
  (test_m1v1_rf_y_pred_tp01 + test_m1v1_rf_y_pred_fn01 + test_m1v1_rf_y_pred_fp01 + test_m1v1_rf_y_pred_tn01)

test_m1v1_rf_err_rate01 <- 1 - test_m1v1_rf_acc01
test_m1v1_rf_recall01 <- test_m1v1_rf_y_pred_tp01 / (test_m1v1_rf_y_pred_tp01 + test_m1v1_rf_y_pred_fn01)
test_m1v1_rf_tnr01 <- test_m1v1_rf_y_pred_tn01 / (test_m1v1_rf_y_pred_tn01 + test_m1v1_rf_y_pred_fp01)
test_m1v1_rf_prec01 <- test_m1v1_rf_y_pred_tp01 / (test_m1v1_rf_y_pred_tp01 + test_m1v1_rf_y_pred_fp01)
test_m1v1_rf_f101 <- (1 + 1^2) * ((test_m1v1_rf_prec01 * test_m1v1_rf_recall01) /
                                    ((1^2 * test_m1v1_rf_prec01) + test_m1v1_rf_recall01))
test_m1v1_rf_plr01 <- test_m1v1_rf_recall01 / (1 - test_m1v1_rf_tnr01)


train_m1v1_rf_acc01 <- (train_m1v1_rf_y_pred_tp01 + train_m1v1_rf_y_pred_tn01) /
  (train_m1v1_rf_y_pred_tp01 + train_m1v1_rf_y_pred_fn01 + train_m1v1_rf_y_pred_fp01 + train_m1v1_rf_y_pred_tn01)

train_m1v1_rf_err_rate01 <- 1 - train_m1v1_rf_acc01
train_m1v1_rf_recall01 <- train_m1v1_rf_y_pred_tp01 / (train_m1v1_rf_y_pred_tp01 + train_m1v1_rf_y_pred_fn01)
train_m1v1_rf_tnr01 <- train_m1v1_rf_y_pred_tn01 / (train_m1v1_rf_y_pred_tn01 + train_m1v1_rf_y_pred_fp01)
train_m1v1_rf_prec01 <- train_m1v1_rf_y_pred_tp01 / (train_m1v1_rf_y_pred_tp01 + train_m1v1_rf_y_pred_fp01)
train_m1v1_rf_f101 <- (1 + 1^2) * ((train_m1v1_rf_prec01 * train_m1v1_rf_recall01) /
                                         ((1^2 * train_m1v1_rf_prec01) + train_m1v1_rf_recall01))
train_m1v1_rf_plr01 <- train_m1v1_rf_recall01 / (1 - train_m1v1_rf_tnr01)

# ==============================================================================
# Generate evaluation measures table
measure_m1v1_rf_name <- c("M1: Random Forest DT",
                        "Baseline Accuracy",
                        "Accuracy",
                        "Error Rate",
                        "Sensitivity/TPR/Recall",
                        "Specificity/TNR",
                        "Precision",
                        "F1",
                        "Positive Liklihood Ratio"
                        )

measure_m1v1_rf_val01 <- c("Traing Data set",
                         paste0(round((4204 / 6690) * 100, 1), "%"),
                         paste0(round(train_m1v1_rf_acc01 * 100, 1), "%"),
                         paste0(round(train_m1v1_rf_err_rate01 * 100, 1), "%"),
                         paste0(round(train_m1v1_rf_recall01 * 100, 1), "%"),
                         paste0(round(train_m1v1_rf_tnr01 * 100, 1), "%"),
                         paste0(round(train_m1v1_rf_prec01 * 100, 1), "%"),
                         paste0(round(train_m1v1_rf_f101 * 100, 1), "%"),
                         paste0(round(train_m1v1_rf_plr01 * 1, 2), "")
                         )

measure_m1v1_rf_val02 <- c("Test Data set",
                         paste0(round((1792 / 2867) * 100, 1), "%"),
                         paste0(round(test_m1v1_rf_acc01 * 100, 1), "%"),
                         paste0(round(test_m1v1_rf_err_rate01 * 100, 1), "%"),
                         paste0(round(test_m1v1_rf_recall01 * 100, 1), "%"),
                         paste0(round(test_m1v1_rf_tnr01 * 100, 1), "%"),
                         paste0(round(test_m1v1_rf_prec01 * 100, 1), "%"),
                         paste0(round(test_m1v1_rf_f101 * 100, 1), "%"),
                         paste0(round(test_m1v1_rf_plr01 * 1, 2), "")
                         )

measure_m1v1_rf_tbl <- data.frame(measure_m1v1_rf_name, measure_m1v1_rf_val01, measure_m1v1_rf_val02)
print(measure_m1v1_rf_tbl)

# Confirm confusion matrix results
confusionMatrix(table(test_m1v1_rf_y_pred_df01, test_df01$Target), positive = "1")

print(paste0("Number of 0s in Test Set Original Target = ", length(which(test_df01$Target == 0))))
print(paste0("Number of 0s in Test Set Predicted = ", length(which(test_m1v1_rf_y_pred_df01 == 0))))
print(paste0("Number of 1s in Test Set Original Target = ", length(which(test_df01$Target == 1))))
print(paste0("Number of 1s in Test Set Predicted = ", length(which(test_m1v1_rf_y_pred_df01 == 1))))

test_m1v1_rf_measure_df01 <- data.frame(test_df01$Target, test_m1v1_rf_y_pred_df01)
print(head(test_m1v1_rf_measure_df01))

print(paste0("Number of True 1s (TP) in Test Set = ",
             length(which(test_m1v1_rf_measure_df01$test_df01.Target == 1 &
                            test_m1v1_rf_measure_df01$test_m1v1_rf_y_pred_df01 == 1))))
print(paste0("Number of False 0s (FN) in Test Set = ",
             length(which(test_m1v1_rf_measure_df01$test_df01.Target == 1 &
                            test_m1v1_rf_measure_df01$test_m1v1_rf_y_pred_df01 == 0))))
print(paste0("Number of False 1s (FP) in Test Set = ",
             length(which(test_m1v1_rf_measure_df01$test_df01.Target == 0 &
                            test_m1v1_rf_measure_df01$test_m1v1_rf_y_pred_df01 == 1))))
print(paste0("Number of True 0s (TN) in Test Set = ",
             length(which(test_m1v1_rf_measure_df01$test_df01.Target == 0 &
                            test_m1v1_rf_measure_df01$test_m1v1_rf_y_pred_df01 == 0))))
```


## **Model 2 (M2):** CART DT
```{r}
# Train model
train_m2v1_cart <- rpart(Target~., data = train_xy01_df01)

# Use trained model to predict y on test data
test_m2v1_cart_y_pred_df01 <- predict(object = train_m2v1_cart, newdata = test_x01_df01, type = "class")

# Create contingency table to review results
test_m2v1_cart_y_pred_tbl01 <- table(test_df01$Target, test_m2v1_cart_y_pred_df01)
row.names(test_m2v1_cart_y_pred_tbl01) <- c("Actual: PSI = 0 (-)",
                                              "Actual: PSI = 1 (+)")
colnames(test_m2v1_cart_y_pred_tbl01) <- c("Predicted: PSI = 0 (-)",
                                             "Predicted: PSI = 1 (+)")
test_m2v1_cart_y_pred_tbl01 <- addmargins(A = test_m2v1_cart_y_pred_tbl01,
                                            FUN = list(Total = sum), quiet = TRUE)
print(test_m2v1_cart_y_pred_tbl01)

## Pull values from confusion matrix table
test_m2v1_cart_y_pred_tp01 <- test_m2v1_cart_y_pred_tbl01[2, 2]
test_m2v1_cart_y_pred_fn01 <- test_m2v1_cart_y_pred_tbl01[2, 1]
test_m2v1_cart_y_pred_fp01 <- test_m2v1_cart_y_pred_tbl01[1, 2]
test_m2v1_cart_y_pred_tn01 <- test_m2v1_cart_y_pred_tbl01[1, 1]
print(test_m2v1_cart_y_pred_tp01)
print(test_m2v1_cart_y_pred_fn01)
print(test_m2v1_cart_y_pred_fp01)
print(test_m2v1_cart_y_pred_tn01)

# ------------------------------------------------------------------------------
# Use trained model to predict y on train data
train_m2v1_cart_y_pred_df01 <- predict(object = train_m2v1_cart, newdata = train_x01_df01, type = "class")

# Create contingency table to review results
train_m2v1_cart_y_pred_tbl01 <- table(train_df01$Target, train_m2v1_cart_y_pred_df01)
row.names(train_m2v1_cart_y_pred_tbl01) <- c("Actual: PSI = 0 (-)",
                                               "Actual: PSI = 1 (+)")
colnames(train_m2v1_cart_y_pred_tbl01) <- c("Predicted: PSI = 0 (-)",
                                              "Predicted: PSI = 1 (+)")
train_m2v1_cart_y_pred_tbl01 <- addmargins(A = train_m2v1_cart_y_pred_tbl01,
                                             FUN = list(Total = sum), quiet = TRUE)
print(train_m2v1_cart_y_pred_tbl01)

# Pull values from confusion matrix table
train_m2v1_cart_y_pred_tp01 <- train_m2v1_cart_y_pred_tbl01[2, 2]
train_m2v1_cart_y_pred_fn01 <- train_m2v1_cart_y_pred_tbl01[2, 1]
train_m2v1_cart_y_pred_fp01 <- train_m2v1_cart_y_pred_tbl01[1, 2]
train_m2v1_cart_y_pred_tn01 <- train_m2v1_cart_y_pred_tbl01[1, 1]
print(train_m2v1_cart_y_pred_tp01)
print(train_m2v1_cart_y_pred_fn01)
print(train_m2v1_cart_y_pred_fp01)
print(train_m2v1_cart_y_pred_tn01)

# Generate classification evaluation measures
test_m2v1_cart_acc01 <- (test_m2v1_cart_y_pred_tp01 + test_m2v1_cart_y_pred_tn01) /
  (test_m2v1_cart_y_pred_tp01 + test_m2v1_cart_y_pred_fn01 + test_m2v1_cart_y_pred_fp01 + test_m2v1_cart_y_pred_tn01)

test_m2v1_cart_err_rate01 <- 1 - test_m2v1_cart_acc01
test_m2v1_cart_recall01 <- test_m2v1_cart_y_pred_tp01 / (test_m2v1_cart_y_pred_tp01 + test_m2v1_cart_y_pred_fn01)
test_m2v1_cart_tnr01 <- test_m2v1_cart_y_pred_tn01 / (test_m2v1_cart_y_pred_tn01 + test_m2v1_cart_y_pred_fp01)
test_m2v1_cart_prec01 <- test_m2v1_cart_y_pred_tp01 / (test_m2v1_cart_y_pred_tp01 + test_m2v1_cart_y_pred_fp01)
test_m2v1_cart_f101 <- (1 + 1^2) * ((test_m2v1_cart_prec01 * test_m2v1_cart_recall01) /
                                        ((1^2 * test_m2v1_cart_prec01) + test_m2v1_cart_recall01))
test_m2v1_cart_plr01 <- test_m2v1_cart_recall01 / (1 - test_m2v1_cart_tnr01)


train_m2v1_cart_acc01 <- (train_m2v1_cart_y_pred_tp01 + train_m2v1_cart_y_pred_tn01) /
  (train_m2v1_cart_y_pred_tp01 + train_m2v1_cart_y_pred_fn01 + train_m2v1_cart_y_pred_fp01 + train_m2v1_cart_y_pred_tn01)

train_m2v1_cart_err_rate01 <- 1 - train_m2v1_cart_acc01
train_m2v1_cart_recall01 <- train_m2v1_cart_y_pred_tp01 / (train_m2v1_cart_y_pred_tp01 + train_m2v1_cart_y_pred_fn01)
train_m2v1_cart_tnr01 <- train_m2v1_cart_y_pred_tn01 / (train_m2v1_cart_y_pred_tn01 + train_m2v1_cart_y_pred_fp01)
train_m2v1_cart_prec01 <- train_m2v1_cart_y_pred_tp01 / (train_m2v1_cart_y_pred_tp01 + train_m2v1_cart_y_pred_fp01)
train_m2v1_cart_f101 <- (1 + 1^2) * ((train_m2v1_cart_prec01 * train_m2v1_cart_recall01) /
                                         ((1^2 * train_m2v1_cart_prec01) + train_m2v1_cart_recall01))
train_m2v1_cart_plr01 <- train_m2v1_cart_recall01 / (1 - train_m2v1_cart_tnr01)

# ==============================================================================
# Generate evaluation measures table
measure_m2v1_cart_name <- c("M2: CART DT",
                          "Baseline Accuracy",
                          "Accuracy",
                          "Error Rate",
                          "Sensitivity/TPR/Recall",
                          "Specificity/TNR",
                          "Precision",
                          "F1",
                          "Positive Liklihood Ratio"
                          )

measure_m2v1_cart_val01 <- c("Traing Data set",
                           paste0(round((4204 / 6690) * 100, 1), "%"),
                           paste0(round(train_m2v1_cart_acc01 * 100, 1), "%"),
                           paste0(round(train_m2v1_cart_err_rate01 * 100, 1), "%"),
                           paste0(round(train_m2v1_cart_recall01 * 100, 1), "%"),
                           paste0(round(train_m2v1_cart_tnr01 * 100, 1), "%"),
                           paste0(round(train_m2v1_cart_prec01 * 100, 1), "%"),
                           paste0(round(train_m2v1_cart_f101 * 100, 1), "%"),
                           paste0(round(train_m2v1_cart_plr01 * 1, 2), "")
                           )

measure_m2v1_cart_val02 <- c("Test Data set",
                           paste0(round((1792 / 2867) * 100, 1), "%"),
                           paste0(round(test_m2v1_cart_acc01 * 100, 1), "%"),
                           paste0(round(test_m2v1_cart_err_rate01 * 100, 1), "%"),
                           paste0(round(test_m2v1_cart_recall01 * 100, 1), "%"),
                           paste0(round(test_m2v1_cart_tnr01 * 100, 1), "%"),
                           paste0(round(test_m2v1_cart_prec01 * 100, 1), "%"),
                           paste0(round(test_m2v1_cart_f101 * 100, 1), "%"),
                           paste0(round(test_m2v1_cart_plr01 * 1, 2), "")
                           )

measure_m2v1_cart_tbl <- data.frame(measure_m2v1_cart_name, measure_m2v1_cart_val01, measure_m2v1_cart_val02)
print(measure_m2v1_cart_tbl)

# Confirm confusion matrix results
confusionMatrix(table(test_m2v1_cart_y_pred_df01, test_df01$Target), positive = "1")

print(paste0("Number of 0s in Test Set Original Target = ", length(which(test_df01$Target == 0))))
print(paste0("Number of 0s in Test Set Predicted = ", length(which(test_m2v1_cart_y_pred_df01 == 0))))
print(paste0("Number of 1s in Test Set Original Target = ", length(which(test_df01$Target == 1))))
print(paste0("Number of 1s in Test Set Predicted = ", length(which(test_m2v1_cart_y_pred_df01 == 1))))

test_m2v1_cart_measure_df01 <- data.frame(test_df01$Target, test_m2v1_cart_y_pred_df01)
print(head(test_m2v1_cart_measure_df01))

print(paste0("Number of True 1s (TP) in Test Set = ",
             length(which(test_m2v1_cart_measure_df01$test_df01.Target == 1 &
                            test_m2v1_cart_measure_df01$test_m2v1_cart_y_pred_df01 == 1))))
print(paste0("Number of False 0s (FN) in Test Set = ",
             length(which(test_m2v1_cart_measure_df01$test_df01.Target == 1 &
                            test_m2v1_cart_measure_df01$test_m2v1_cart_y_pred_df01 == 0))))
print(paste0("Number of False 1s (FP) in Test Set = ",
             length(which(test_m2v1_cart_measure_df01$test_df01.Target == 0 &
                            test_m2v1_cart_measure_df01$test_m2v1_cart_y_pred_df01 == 1))))
print(paste0("Number of True 0s (TN) in Test Set = ",
             length(which(test_m2v1_cart_measure_df01$test_df01.Target == 0 &
                            test_m2v1_cart_measure_df01$test_m2v1_cart_y_pred_df01 == 0))))
```


## **Model 3 (M3):** C5.0 Decision Tree
```{r}
# Train model
train_m3v1_c50 <- C5.0(Target~., data = train_xy01_df01, control = C5.0Control(minCases=75))

# Use trained model to predict y on test data
test_m3v1_c50_y_pred_df01 <- predict(object = train_m3v1_c50, newdata = test_x01_df01, type = "class")

# Create contingency table to review results
test_m3v1_c50_y_pred_tbl01 <- table(test_df01$Target, test_m3v1_c50_y_pred_df01)
row.names(test_m3v1_c50_y_pred_tbl01) <- c("Actual: PSI = 0 (-)",
                                             "Actual: PSI = 1 (+)")
colnames(test_m3v1_c50_y_pred_tbl01) <- c("Predicted: PSI = 0 (-)",
                                            "Predicted: PSI = 1 (+)")
test_m3v1_c50_y_pred_tbl01 <- addmargins(A = test_m3v1_c50_y_pred_tbl01,
                                           FUN = list(Total = sum), quiet = TRUE)
print(test_m3v1_c50_y_pred_tbl01)

# Pull values from confusion matrix table
test_m3v1_c50_y_pred_tp01 <- test_m3v1_c50_y_pred_tbl01[2, 2]
test_m3v1_c50_y_pred_fn01 <- test_m3v1_c50_y_pred_tbl01[2, 1]
test_m3v1_c50_y_pred_fp01 <- test_m3v1_c50_y_pred_tbl01[1, 2]
test_m3v1_c50_y_pred_tn01 <- test_m3v1_c50_y_pred_tbl01[1, 1]
print(test_m3v1_c50_y_pred_tp01)
print(test_m3v1_c50_y_pred_fn01)
print(test_m3v1_c50_y_pred_fp01)
print(test_m3v1_c50_y_pred_tn01)

# ------------------------------------------------------------------------------
# Use trained model to predict y on train data
train_m3v1_c50_y_pred_df01 <- predict(object = train_m3v1_c50, newdata = train_x01_df01, type = "class")

# Create contingency table to review results
train_m3v1_c50_y_pred_tbl01 <- table(train_df01$Target, train_m3v1_c50_y_pred_df01)
row.names(train_m3v1_c50_y_pred_tbl01) <- c("Actual: PSI = 0 (-)",
                                              "Actual: PSI = 1 (+)")
colnames(train_m3v1_c50_y_pred_tbl01) <- c("Predicted: PSI = 0 (-)",
                                             "Predicted: PSI = 1 (+)")
train_m3v1_c50_y_pred_tbl01 <- addmargins(A = train_m3v1_c50_y_pred_tbl01,
                                            FUN = list(Total = sum), quiet = TRUE)
print(train_m3v1_c50_y_pred_tbl01)

# Pull values from confusion matrix table
train_m3v1_c50_y_pred_tp01 <- train_m3v1_c50_y_pred_tbl01[2, 2]
train_m3v1_c50_y_pred_fn01 <- train_m3v1_c50_y_pred_tbl01[2, 1]
train_m3v1_c50_y_pred_fp01 <- train_m3v1_c50_y_pred_tbl01[1, 2]
train_m3v1_c50_y_pred_tn01 <- train_m3v1_c50_y_pred_tbl01[1, 1]
print(train_m3v1_c50_y_pred_tp01)
print(train_m3v1_c50_y_pred_fn01)
print(train_m3v1_c50_y_pred_fp01)
print(train_m3v1_c50_y_pred_tn01)

# Generate classification evaluation measures
test_m3v1_c50_acc01 <- (test_m3v1_c50_y_pred_tp01 + test_m3v1_c50_y_pred_tn01) /
  (test_m3v1_c50_y_pred_tp01 + test_m3v1_c50_y_pred_fn01 + test_m3v1_c50_y_pred_fp01 + test_m3v1_c50_y_pred_tn01)

test_m3v1_c50_err_rate01 <- 1 - test_m3v1_c50_acc01
test_m3v1_c50_recall01 <- test_m3v1_c50_y_pred_tp01 / (test_m3v1_c50_y_pred_tp01 + test_m3v1_c50_y_pred_fn01)
test_m3v1_c50_tnr01 <- test_m3v1_c50_y_pred_tn01 / (test_m3v1_c50_y_pred_tn01 + test_m3v1_c50_y_pred_fp01)
test_m3v1_c50_prec01 <- test_m3v1_c50_y_pred_tp01 / (test_m3v1_c50_y_pred_tp01 + test_m3v1_c50_y_pred_fp01)
test_m3v1_c50_f101 <- (1 + 1^2) * ((test_m3v1_c50_prec01 * test_m3v1_c50_recall01) /
                                       ((1^2 * test_m3v1_c50_prec01) + test_m3v1_c50_recall01))
test_m3v1_c50_plr01 <- test_m3v1_c50_recall01 / (1 - test_m3v1_c50_tnr01)


train_m3v1_c50_acc01 <- (train_m3v1_c50_y_pred_tp01 + train_m3v1_c50_y_pred_tn01) /
  (train_m3v1_c50_y_pred_tp01 + train_m3v1_c50_y_pred_fn01 + train_m3v1_c50_y_pred_fp01 + train_m3v1_c50_y_pred_tn01)

train_m3v1_c50_err_rate01 <- 1 - train_m3v1_c50_acc01
train_m3v1_c50_recall01 <- train_m3v1_c50_y_pred_tp01 / (train_m3v1_c50_y_pred_tp01 + train_m3v1_c50_y_pred_fn01)
train_m3v1_c50_tnr01 <- train_m3v1_c50_y_pred_tn01 / (train_m3v1_c50_y_pred_tn01 + train_m3v1_c50_y_pred_fp01)
train_m3v1_c50_prec01 <- train_m3v1_c50_y_pred_tp01 / (train_m3v1_c50_y_pred_tp01 + train_m3v1_c50_y_pred_fp01)
train_m3v1_c50_f101 <- (1 + 1^2) * ((train_m3v1_c50_prec01 * train_m3v1_c50_recall01) /
                                        ((1^2 * train_m3v1_c50_prec01) + train_m3v1_c50_recall01))
train_m3v1_c50_plr01 <- train_m3v1_c50_recall01 / (1 - train_m3v1_c50_tnr01)

# ==============================================================================
# Generate evaluation measures table
measure_m3v1_c50_name <- c("M3: C5.0 DT",
                         "Baseline Accuracy",
                         "Accuracy", "Error Rate",
                         "Sensitivity/TPR/Recall",
                         "Specificity/TNR",
                         "Precision",
                         "F1",
                         "Positive Liklihood Ratio"
                         )

measure_m3v1_c50_val01 <- c("Traing Data set",
                          paste0(round((4204 / 6690) * 100, 1), "%"),
                          paste0(round(train_m3v1_c50_acc01 * 100, 1), "%"),
                          paste0(round(train_m3v1_c50_err_rate01 * 100, 1), "%"),
                          paste0(round(train_m3v1_c50_recall01 * 100, 1), "%"),
                          paste0(round(train_m3v1_c50_tnr01 * 100, 1), "%"),
                          paste0(round(train_m3v1_c50_prec01 * 100, 1), "%"),
                          paste0(round(train_m3v1_c50_f101 * 100, 1), "%"),
                          paste0(round(train_m3v1_c50_plr01 * 1, 2), "")
                          )

measure_m3v1_c50_val02 <- c("Test Data set",
                          paste0(round((1792 / 2867) * 100, 1), "%"),
                          paste0(round(test_m3v1_c50_acc01 * 100, 1), "%"),
                          paste0(round(test_m3v1_c50_err_rate01 * 100, 1), "%"),
                          paste0(round(test_m3v1_c50_recall01 * 100, 1), "%"),
                          paste0(round(test_m3v1_c50_tnr01 * 100, 1), "%"),
                          paste0(round(test_m3v1_c50_prec01 * 100, 1), "%"),
                          paste0(round(test_m3v1_c50_f101 * 100, 1), "%"),
                          paste0(round(test_m3v1_c50_plr01 * 1, 2), "")
                          )

measure_m3v1_c50_tbl <- data.frame(measure_m3v1_c50_name, measure_m3v1_c50_val01, measure_m3v1_c50_val02)
print(measure_m3v1_c50_tbl)

# Confirm confusion matrix results
confusionMatrix(table(test_m3v1_c50_y_pred_df01, test_df01$Target), positive = "1")

print(paste0("Number of 0s in Test Set Original Target = ", length(which(test_df01$Target == 0))))
print(paste0("Number of 0s in Test Set Predicted = ", length(which(test_m3v1_c50_y_pred_df01 == 0))))
print(paste0("Number of 1s in Test Set Original Target = ", length(which(test_df01$Target == 1))))
print(paste0("Number of 1s in Test Set Predicted = ", length(which(test_m3v1_c50_y_pred_df01 == 1))))

test_m3v1_c50_measure_df01 <- data.frame(test_df01$Target, test_m3v1_c50_y_pred_df01)
print(head(test_m3v1_c50_measure_df01))

print(paste0("Number of True 1s (TP) in Test Set = ",
             length(which(test_m3v1_c50_measure_df01$test_df01.Target == 1 &
                            test_m3v1_c50_measure_df01$test_m3v1_c50_y_pred_df01 == 1))))
print(paste0("Number of False 0s (FN) in Test Set = ",
             length(which(test_m3v1_c50_measure_df01$test_df01.Target == 1 &
                            test_m3v1_c50_measure_df01$test_m3v1_c50_y_pred_df01 == 0))))
print(paste0("Number of False 1s (FP) in Test Set = ",
             length(which(test_m3v1_c50_measure_df01$test_df01.Target == 0 &
                            test_m3v1_c50_measure_df01$test_m3v1_c50_y_pred_df01 == 1))))
print(paste0("Number of True 0s (TN) in Test Set = ",
             length(which(test_m3v1_c50_measure_df01$test_df01.Target == 0 &
                            test_m3v1_c50_measure_df01$test_m3v1_c50_y_pred_df01 == 0))))
```


## **Model 4 (M4):** *K*-Nearest Neighbor (KNN), *k* = 81
```{r}
# Train model
test_m4v1_knn81 <- knn(train_x01_df01, test_x01_df01, cl = train_xy01_df01$Target, k = 81, prob = TRUE)

# Create contingency table to review results
test_m4v1_knn81_y_pred_tbl01 <- table(test_df01$Target, test_m4v1_knn81)

row.names(test_m4v1_knn81_y_pred_tbl01) <- c("Actual: PSI = 0 (-)",
                                               "Actual: PSI = 1 (+)")
colnames(test_m4v1_knn81_y_pred_tbl01) <- c("Predicted: PSI = 0 (-)",
                                              "Predicted: PSI = 1 (+)")
test_m4v1_knn81_y_pred_tbl01 <- addmargins(A = test_m4v1_knn81_y_pred_tbl01,
                                             FUN = list(Total = sum), quiet = TRUE)
print(test_m4v1_knn81_y_pred_tbl01)

# Pull values from confusion matrix table
test_m4v1_knn81_y_pred_tp01 <- test_m4v1_knn81_y_pred_tbl01[2, 2]
test_m4v1_knn81_y_pred_fn01 <- test_m4v1_knn81_y_pred_tbl01[2, 1]
test_m4v1_knn81_y_pred_fp01 <- test_m4v1_knn81_y_pred_tbl01[1, 2]
test_m4v1_knn81_y_pred_tn01 <- test_m4v1_knn81_y_pred_tbl01[1, 1]
print(test_m4v1_knn81_y_pred_tp01)
print(test_m4v1_knn81_y_pred_fn01)
print(test_m4v1_knn81_y_pred_fp01)
print(test_m4v1_knn81_y_pred_tn01)

# Generate classification evaluation measures
test_m4v1_knn81_acc01 <- (test_m4v1_knn81_y_pred_tp01 + test_m4v1_knn81_y_pred_tn01) /
  (test_m4v1_knn81_y_pred_tp01 + test_m4v1_knn81_y_pred_fn01 + test_m4v1_knn81_y_pred_fp01 + test_m4v1_knn81_y_pred_tn01)
test_m4v1_knn81_err_rate01 <- 1 - test_m4v1_knn81_acc01
test_m4v1_knn81_recall01 <- test_m4v1_knn81_y_pred_tp01 /
  (test_m4v1_knn81_y_pred_tp01 + test_m4v1_knn81_y_pred_fn01)
test_m4v1_knn81_tnr01 <- test_m4v1_knn81_y_pred_tn01 /
  (test_m4v1_knn81_y_pred_tn01 + test_m4v1_knn81_y_pred_fp01)
test_m4v1_knn81_prec01 <- test_m4v1_knn81_y_pred_tp01 /
  (test_m4v1_knn81_y_pred_tp01 + test_m4v1_knn81_y_pred_fp01)
test_m4v1_knn81_f101 <- (1 + 1^2) * ((test_m4v1_knn81_prec01 * test_m4v1_knn81_recall01) /
                                       ((1^2 * test_m4v1_knn81_prec01) + test_m4v1_knn81_recall01))
test_m4v1_knn81_plr01 <- test_m4v1_knn81_recall01 /
  (1 - test_m4v1_knn81_tnr01)

# ==============================================================================
# Generate evaluation measures table
measure_test_m4v1_knn81_name <- c("M4: KNN-81",
                                "Accuracy",
                                "Error Rate",
                                "Sensitivity/TPR/Recall",
                                "Specificity/TNR",
                                "Precision",
                                "F1",
                                "Positive Liklihood Ratio"
                                )

measure_test_m4v1_knn81_val02 <- c("Eval. Measure Values",
                                 paste0(round(test_m4v1_knn81_acc01 * 100, 1), "%"),
                                 paste0(round(test_m4v1_knn81_err_rate01 * 100, 1), "%"),
                                 paste0(round(test_m4v1_knn81_recall01 * 100, 1), "%"),
                                 paste0(round(test_m4v1_knn81_tnr01 * 100, 1), "%"),
                                 paste0(round(test_m4v1_knn81_prec01 * 100, 1), "%"),
                                 paste0(round(test_m4v1_knn81_f101 * 100, 1), "%"),
                                 paste0(round(test_m4v1_knn81_plr01 * 1, 2), "")
                                 )

measure_test_m4v1_knn81_tbl <- data.frame(measure_test_m4v1_knn81_name, measure_test_m4v1_knn81_val02)
print(measure_test_m4v1_knn81_tbl)

# Confirm confusion matrix results
confusionMatrix(table(test_m4v1_knn81, test_df01$Target), positive = "1")

# Plot receiver operator characteristic (ROC) curve
test_m4v1_knn81_df <- attr(test_m4v1_knn81, which = "prob")
test_m4v1_knn81_pred <- prediction(test_m4v1_knn81_df, test_df01$Target, label.ordering = c("1", "0"))
test_m4v1_knn81_roc <- performance(test_m4v1_knn81_pred, measure = "tpr", x.measure = "fpr")
plot(test_m4v1_knn81_roc)

# Calculate area under curve (AUC)
test_m4v1_knn81_auc <- performance(test_m4v1_knn81_pred, measure = "auc")
test_m4v1_knn81_auc <- test_m4v1_knn81_auc@y.values[[1]]
print(test_m4v1_knn81_auc)
```


## **Model 5 (M5):** KNN, *k* = 3
```{r}
# Train model
test_m5v1_knn03 <- knn(train_x01_df01, test_x01_df01, cl = train_xy01_df01$Target, k = 3, prob = TRUE)

# Create contingency table to review results
test_m5v1_knn03_y_pred_tbl01 <- table(test_df01$Target, test_m5v1_knn03)

row.names(test_m5v1_knn03_y_pred_tbl01) <- c("Actual: PSI = 0 (-)",
                                               "Actual: PSI = 1 (+)")
colnames(test_m5v1_knn03_y_pred_tbl01) <- c("Predicted: PSI = 0 (-)",
                                              "Predicted: PSI = 1 (+)")
test_m5v1_knn03_y_pred_tbl01 <- addmargins(A = test_m5v1_knn03_y_pred_tbl01,
                                             FUN = list(Total = sum), quiet = TRUE)
print(test_m5v1_knn03_y_pred_tbl01)

# Pull values from confusion matrix table
test_m5v1_knn03_y_pred_tp01 <- test_m5v1_knn03_y_pred_tbl01[2, 2]
test_m5v1_knn03_y_pred_fn01 <- test_m5v1_knn03_y_pred_tbl01[2, 1]
test_m5v1_knn03_y_pred_fp01 <- test_m5v1_knn03_y_pred_tbl01[1, 2]
test_m5v1_knn03_y_pred_tn01 <- test_m5v1_knn03_y_pred_tbl01[1, 1]
print(test_m5v1_knn03_y_pred_tp01)
print(test_m5v1_knn03_y_pred_fn01)
print(test_m5v1_knn03_y_pred_fp01)
print(test_m5v1_knn03_y_pred_tn01)

# Generate classification evaluation measures
test_m5v1_knn03_acc01 <- (test_m5v1_knn03_y_pred_tp01 + test_m5v1_knn03_y_pred_tn01) /
  (test_m5v1_knn03_y_pred_tp01 + test_m5v1_knn03_y_pred_fn01 + test_m5v1_knn03_y_pred_fp01 + test_m5v1_knn03_y_pred_tn01)
test_m5v1_knn03_err_rate01 <- 1 - test_m5v1_knn03_acc01
test_m5v1_knn03_recall01 <- test_m5v1_knn03_y_pred_tp01 /
  (test_m5v1_knn03_y_pred_tp01 + test_m5v1_knn03_y_pred_fn01)
test_m5v1_knn03_tnr01 <- test_m5v1_knn03_y_pred_tn01 /
  (test_m5v1_knn03_y_pred_tn01 + test_m5v1_knn03_y_pred_fp01)
test_m5v1_knn03_prec01 <- test_m5v1_knn03_y_pred_tp01 /
  (test_m5v1_knn03_y_pred_tp01 + test_m5v1_knn03_y_pred_fp01)
test_m5v1_knn03_f101 <- (1 + 1^2) * ((test_m5v1_knn03_prec01 * test_m5v1_knn03_recall01) /
                                       ((1^2 * test_m5v1_knn03_prec01) + test_m5v1_knn03_recall01))
test_m5v1_knn03_plr01 <- test_m5v1_knn03_recall01 /
  (1 - test_m5v1_knn03_tnr01)

# ==============================================================================
# Generate evaluation measures table
measure_test_m5v1_knn03_name <- c("M5: KNN-3",
                                "Accuracy",
                                "Error Rate",
                                "Sensitivity/TPR/Recall",
                                "Specificity/TNR",
                                "Precision",
                                "F1",
                                "Positive Liklihood Ratio"
                                )

measure_test_m5v1_knn03_val02 <- c("Eval. Measure Values",
                                 paste0(round(test_m5v1_knn03_acc01 * 100, 1), "%"),
                                 paste0(round(test_m5v1_knn03_err_rate01 * 100, 1), "%"),
                                 paste0(round(test_m5v1_knn03_recall01 * 100, 1), "%"),
                                 paste0(round(test_m5v1_knn03_tnr01 * 100, 1), "%"),
                                 paste0(round(test_m5v1_knn03_prec01 * 100, 1), "%"),
                                 paste0(round(test_m5v1_knn03_f101 * 100, 1), "%"),
                                 paste0(round(test_m5v1_knn03_plr01 * 1, 2), "")
                                 )

measure_test_m5v1_knn03_tbl <- data.frame(measure_test_m5v1_knn03_name, measure_test_m5v1_knn03_val02)
print(measure_test_m5v1_knn03_tbl)

# Confirm confusion matrix results
confusionMatrix(table(test_m5v1_knn03, test_df01$Target), positive = "1")

# Plot receiver operator characteristic (ROC) curve
test_m5v1_knn03_df <- attr(test_m5v1_knn03, which = "prob")
test_m5v1_knn03_pred <- prediction(test_m5v1_knn03_df, test_df01$Target, label.ordering = c("1", "0"))
test_m5v1_knn03_roc <- performance(test_m5v1_knn03_pred, measure = "tpr", x.measure = "fpr")
plot(test_m5v1_knn03_roc)

# Calculate area under curve (AUC)
test_m5v1_knn03_auc <- performance(test_m5v1_knn03_pred, measure = "auc")
test_m5v1_knn03_auc <- test_m5v1_knn03_auc@y.values[[1]]
print(test_m5v1_knn03_auc)
```
